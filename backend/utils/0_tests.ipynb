{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import orjson\n",
    "import traceback\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from typing import DefaultDict, Dict, List\n",
    "from common import (\n",
    "    read_binary_file,\n",
    "    get_preprocessed_words,\n",
    "    load_batch_from_news_source,\n",
    "    save_json_file,\n",
    "    load_json_file,\n",
    "    get_indices_for_news_data,\n",
    ")\n",
    "from basetype import (\n",
    "    InvertedIndex,\n",
    "    InvertedIndexMetadata,\n",
    "    NewsArticleData,\n",
    "    NewsArticlesFragment,\n",
    "    NewsArticlesBatch,\n",
    "    default_dict_list,\n",
    ")\n",
    "from constant import Source, CHILD_INDEX_PATH, GLOBAL_INDEX_PATH\n",
    "from datetime import date\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "CURRENT_DIR = os.getcwd()\n",
    "NUM_OF_CORES = os.cpu_count() or 1\n",
    "\n",
    "\n",
    "def process_batch(\n",
    "    fragment_list: List[NewsArticlesFragment],\n",
    "    inverted_index: InvertedIndex,\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    ") -> None:\n",
    "    local_index = defaultdict(dict)\n",
    "    for fragment in fragment_list:\n",
    "        for article in fragment.articles:\n",
    "            doc_id = article.doc_id\n",
    "            doc_text = article.title + \"\\n\" + article.content\n",
    "            text_words = get_preprocessed_words(doc_text, stopping, stemming)\n",
    "            for position, word in enumerate(text_words):\n",
    "                if doc_id not in local_index[word]:\n",
    "                    local_index[word][doc_id] = []\n",
    "                local_index[word][doc_id].append(position + 1)\n",
    "    try:\n",
    "        for word in local_index:\n",
    "            for doc_id in local_index[word]:\n",
    "                inverted_index.index[word][doc_id] += local_index[word][doc_id]\n",
    "    except:\n",
    "        print(\"Error processing batch\")\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "\n",
    "\n",
    "def positional_inverted_index(\n",
    "    news_batch: NewsArticlesBatch,\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    ") -> InvertedIndex:\n",
    "    doc_ids = news_batch.doc_ids\n",
    "    document_size = len(doc_ids)\n",
    "    inverted_index_meta = InvertedIndexMetadata(\n",
    "        document_size=document_size, doc_ids_list=doc_ids\n",
    "    )\n",
    "\n",
    "    inverted_index = InvertedIndex(\n",
    "        meta=inverted_index_meta, index=defaultdict(default_dict_list)\n",
    "    )\n",
    "\n",
    "    # cut the fragments into batches\n",
    "    for source, fragments in news_batch.fragments.items():\n",
    "        curr_time = time.time()\n",
    "        batch_size = len(fragments) // NUM_OF_CORES\n",
    "        remainder = len(fragments) % NUM_OF_CORES\n",
    "        batches = [\n",
    "            fragments[i * batch_size : (i + 1) * batch_size]\n",
    "            for i in range(NUM_OF_CORES)\n",
    "        ]\n",
    "        if remainder != 0:\n",
    "            # append the remainder to the last batch\n",
    "            batches[-1] += fragments[-remainder:]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=NUM_OF_CORES) as executor:\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    process_batch, batch, inverted_index, stopping, stemming\n",
    "                )\n",
    "                for batch in batches\n",
    "            ]\n",
    "\n",
    "        for future in futures:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                traceback.print_exc()\n",
    "                exit()\n",
    "\n",
    "        print(\n",
    "            f\"Time taken for processing {source}: {time.time() - curr_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# save as binary file\n",
    "def save_index_file(\n",
    "    file_name: str,\n",
    "    index: DefaultDict[str, Dict[str, list]],\n",
    "    output_dir: str = \"binary_file\",\n",
    "):\n",
    "    if not os.path.exists(os.path.join(CURRENT_DIR, output_dir)):\n",
    "        os.mkdir(os.path.join(CURRENT_DIR, output_dir))\n",
    "    # sort index by term and doc_id in int\n",
    "    index_output = dict(sorted(index.items()))\n",
    "    for term, record in index_output.items():\n",
    "        if term == \"document_size\" or term == \"doc_ids_list\":\n",
    "            continue\n",
    "        index_output[term] = dict(sorted(record.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, file_name), \"wb\") as f:\n",
    "        for term, record in index_output.items():\n",
    "            if term == \"document_size\" or term == \"doc_ids_list\":\n",
    "                continue\n",
    "            f.write(f\"{term} {len(record)}\\n\".encode(\"utf8\"))\n",
    "            for doc_id, positions in record.items():\n",
    "                f.write(\n",
    "                    f\"\\t{doc_id}: {','.join([str(pos) for pos in positions])}\\n\".encode(\n",
    "                        \"utf8\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "def load_binary_index(file_name: str, output_dir: str = \"binary_file\") -> dict:\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, file_name), \"rb\") as f:\n",
    "        data = f.read().decode(\"utf8\")\n",
    "    return orjson.loads(data)\n",
    "\n",
    "\n",
    "def merge_inverted_indices(\n",
    "    global_index: DefaultDict[str, DefaultDict[str, List[int]]],\n",
    "    child_index: DefaultDict[str, DefaultDict[str, List[int]]],\n",
    "):\n",
    "\n",
    "    if not global_index:\n",
    "        global_index.update(child_index)\n",
    "        return\n",
    "\n",
    "    child_index_set = set(child_index.keys())\n",
    "    global_index_set = set(global_index.keys())\n",
    "    new_keys = child_index_set - global_index_set\n",
    "    common_keys = child_index_set & global_index_set\n",
    "\n",
    "    # the docID must be new!\n",
    "    for key in new_keys:\n",
    "        global_index[key] = child_index[key]\n",
    "\n",
    "    for key in common_keys:\n",
    "        for doc_id in child_index[key]:\n",
    "            if doc_id not in global_index[key]:\n",
    "                global_index[key][doc_id] = child_index[key][doc_id]\n",
    "            elif doc_id in global_index[key]:\n",
    "                print(\n",
    "                    \"WARNING: Trying to add new documents under the same doc ID!\",\n",
    "                    key,\n",
    "                    doc_id,\n",
    "                )\n",
    "\n",
    "\n",
    "def delta_encode_list(positions):\n",
    "    \"\"\"Convert a list of positions into a delta-encoded list.\"\"\"\n",
    "    if not positions:\n",
    "        return []\n",
    "    # The first position remains the same, others are differences from the previous one\n",
    "    delta_encoded = [positions[0]] + [\n",
    "        positions[i] - positions[i - 1] for i in range(1, len(positions))\n",
    "    ]\n",
    "    return delta_encoded\n",
    "\n",
    "\n",
    "def delta_decode_list(delta_encoded):\n",
    "    \"\"\"Reconstruct the original list of positions from a delta-encoded list.\"\"\"\n",
    "    positions = [delta_encoded[0]] if delta_encoded else []\n",
    "    for delta in delta_encoded[1:]:\n",
    "        positions.append(positions[-1] + delta)\n",
    "    return positions\n",
    "\n",
    "\n",
    "# old\n",
    "# def encode_index(index: DefaultDict[str, Dict[str, list]]):\n",
    "#     for term, record in index.items():\n",
    "#         for doc_id, positions in record.items():\n",
    "#             index[term][doc_id] = delta_encode_list(positions)\n",
    "\n",
    "\n",
    "def encode_index(inverted_index: InvertedIndex):\n",
    "    # Delta encode the positions\n",
    "    for term, record in inverted_index.index.items():\n",
    "        for doc_id, positions in record.items():\n",
    "            inverted_index.index[term][doc_id] = delta_encode_list(positions)\n",
    "    # Delta encode postings list\n",
    "    inverted_index.meta.doc_ids_list = delta_encode_list(\n",
    "        inverted_index.meta.doc_ids_list\n",
    "    )\n",
    "\n",
    "\n",
    "def decode_index(inverted_index: InvertedIndex):\n",
    "    # Decode the positions\n",
    "    for term, record in inverted_index.index.items():\n",
    "        for doc_id, positions in record.items():\n",
    "            inverted_index.index[term][doc_id] = delta_decode_list(positions)\n",
    "    # Decode postings list\n",
    "    inverted_index.meta.doc_ids_list = delta_decode_list(\n",
    "        inverted_index.meta.doc_ids_list\n",
    "    )\n",
    "\n",
    "\n",
    "def build_child_index(\n",
    "    source: Source,\n",
    "    date: date,\n",
    "    interval=10,\n",
    "):\n",
    "    # file name format: {source_name}_{YYYY-MM-DD}_{start_number}_{end_number}.json\n",
    "    time_str = date.strftime(\"%Y-%m-%d\")\n",
    "    pattern = re.compile(f\"{source.value}_{time_str}_([0-9]+)_([0-9]+).json\")\n",
    "    last_index = -1\n",
    "    if os.path.exists(CHILD_INDEX_PATH):\n",
    "        child_index_file_list = [\n",
    "            file for file in os.listdir(CHILD_INDEX_PATH) if pattern.match(file)\n",
    "        ]\n",
    "        print(child_index_file_list)\n",
    "        for file in child_index_file_list:\n",
    "            # split by .csv\n",
    "            file_name = file.split(\".\")[0]\n",
    "            # split by _\n",
    "            file_info = file_name.split(\"_\")\n",
    "            if int(file_info[-1]) > last_index:\n",
    "                last_index = int(file_info[-1])\n",
    "\n",
    "    indices = get_indices_for_news_data(source.value, date)\n",
    "\n",
    "    # prune the indices\n",
    "    indices = [index for index in indices if index > last_index]\n",
    "\n",
    "    # divide the indices into intervals\n",
    "    indices_batches = [\n",
    "        indices[i : i + interval] for i in range(0, len(indices), interval)\n",
    "    ]\n",
    "    index_list = []\n",
    "    for indices_batch in indices_batches:\n",
    "        news_batch = load_batch_from_news_source(\n",
    "            source, date, indices_batch[0], indices_batch[-1]\n",
    "        )\n",
    "        inverted_index = positional_inverted_index(news_batch)\n",
    "        encode_index(inverted_index)\n",
    "        save_json_file(\n",
    "            f\"{source.value}_{date}_{indices_batch[0]}_{indices_batch[-1]}.json\",\n",
    "            inverted_index.model_dump(),\n",
    "            \"index/child\",\n",
    "        )\n",
    "        index_list.append(inverted_index)\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Loading tele data 20240216_9.csv                                                                    \n",
      "Time taken for processing tele: 1.10 seconds\n",
      "Loading tele data 20240216_10.csv                                                                   \n",
      "Time taken for processing tele: 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    (Source.TELE, date(2024, 2, 16))\n",
    "]\n",
    "\n",
    "index_list = build_child_index(Source.TELE, date(2024, 2, 16))\n",
    "\n",
    "test_index = index_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'310427': [2, 278],\n",
       "             '310430': [240, 341],\n",
       "             '310732': [25],\n",
       "             '310737': [119],\n",
       "             '310739': [427],\n",
       "             '311034': [37, 18, 9, 16, 23, 104, 49, 28, 97],\n",
       "             '311041': [82],\n",
       "             '311333': [280, 49],\n",
       "             '311339': [77],\n",
       "             '311635': [378, 404],\n",
       "             '311640': [47],\n",
       "             '312237': [233],\n",
       "             '312538': [292],\n",
       "             '312839': [404]})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index.index['action']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttds-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
