{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from xml.dom import minidom\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import traceback\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "STOP_WORDS_FILE = \"ttds_2023_english_stop_words.txt\"\n",
    "XML_FILES = [\"sample.xml\", \"trec.sample.xml\", \"trec.5000.xml\"]\n",
    "# CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "CURRENT_DIR = os.getcwd()\n",
    "NUM_OF_CORES = os.cpu_count()\n",
    "\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name: str) -> str:\n",
    "    with open(os.path.join(CURRENT_DIR, file_name), \"r\", encoding=\"utf8\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "read_xml_file(\"trec.sample.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name: str) -> str:\n",
    "    with open(os.path.join(CURRENT_DIR, file_name), \"r\", encoding=\"utf8\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_xml_file(file_name: str) -> minidom.Document:\n",
    "    file = minidom.parse(file_name)\n",
    "    return file\n",
    "\n",
    "\n",
    "def get_stop_words(file_name: str = STOP_WORDS_FILE) -> list:\n",
    "    assert os.path.exists(\n",
    "        os.path.join(CURRENT_DIR, file_name)\n",
    "    ), f\"File {file_name} does not exist\"\n",
    "    with open(os.path.join(CURRENT_DIR, file_name), \"r\") as f:\n",
    "        stop_words = f.read()\n",
    "    return stop_words.split(\"\\n\")\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens: list) -> list:\n",
    "    assert os.path.exists(\n",
    "        os.path.join(CURRENT_DIR, STOP_WORDS_FILE)\n",
    "    ), f\"File {STOP_WORDS_FILE} does not exist\"\n",
    "    stop_words = get_stop_words(STOP_WORDS_FILE)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "def tokenize(content: str) -> list:\n",
    "    return re.findall(r\"\\w+\", content)\n",
    "\n",
    "def get_stemmed_words(tokens: list) -> list:\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(token) for token in tokens]\n",
    "    return words\n",
    "\n",
    "def replace_non_word_characters(content: str) -> str:\n",
    "    # replace non word characters with space\n",
    "    return re.sub(r\"[^\\w\\s]\", \" \", content)\n",
    "\n",
    "def get_preprocessed_words(\n",
    "    content: str, stopping: bool = True, stemming: bool = True\n",
    ") -> list:\n",
    "    tokens = tokenize(content)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    if stopping:\n",
    "        tokens = remove_stop_words(tokens)\n",
    "    if stemming:\n",
    "        tokens = get_stemmed_words(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess_match(\n",
    "    match: re.Match, stopping: bool = True, stemming: bool = True\n",
    ") -> str:\n",
    "    word = match.group(0)\n",
    "    if word in [\"AND\", \"OR\", \"NOT\"]:\n",
    "        return word\n",
    "    word = word.lower()\n",
    "\n",
    "    stopwords = get_stop_words()\n",
    "    if stopping and word in stopwords:\n",
    "        return \"\"\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        word = stemmer.stem(word)\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def save_json_file(file_name: str, data: dict, output_dir: str = \"result\"):\n",
    "    if not os.path.exists(os.path.join(CURRENT_DIR, output_dir)):\n",
    "        os.mkdir(os.path.join(CURRENT_DIR, output_dir))\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, file_name), \"wb\") as f:\n",
    "        f.write(json.dumps(data).encode(\"utf8\"))\n",
    "\n",
    "\n",
    "def index_docs(\n",
    "    docs_batches: minidom.Document,\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    "    escape_char: bool = False,\n",
    "    headline: bool = False,\n",
    ") -> defaultdict(dict):\n",
    "    local_index = defaultdict(dict)\n",
    "    try:\n",
    "        for doc in docs_batches:\n",
    "            doc_id = (\n",
    "                doc.find(\"docno\").text\n",
    "                if not escape_char\n",
    "                else doc.find(\"docno\").decode_contents()\n",
    "            )\n",
    "            doc_text = (\n",
    "                doc.find(\"text\").text\n",
    "                if not escape_char\n",
    "                else doc.find(\"text\").decode_contents()\n",
    "            )\n",
    "\n",
    "            text_words = get_preprocessed_words(doc_text, stopping, stemming)\n",
    "            if headline:\n",
    "                headline = (\n",
    "                    doc.find(\"headline\").text\n",
    "                    if not escape_char\n",
    "                    else doc.find(\"headline\").decode_contents()\n",
    "                )\n",
    "                headline_words = get_preprocessed_words(headline, stopping, stemming)\n",
    "                text_words = headline_words + text_words\n",
    "\n",
    "            for position, word in enumerate(text_words):\n",
    "                if doc_id not in local_index[word]:\n",
    "                    local_index[word][doc_id] = []\n",
    "                local_index[word][doc_id].append(position + 1)\n",
    "    except:\n",
    "        print(\"Error processing doc_id\", doc_id)\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "\n",
    "    return local_index\n",
    "\n",
    "\n",
    "def process_batch(\n",
    "    docs_batch: list,\n",
    "    pos_inverted_index: defaultdict(dict),\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    "    escape_char: bool = False,\n",
    "    headline: bool = False,\n",
    "):\n",
    "    local_index = index_docs(docs_batch, stopping, stemming, escape_char, headline)\n",
    "    try:\n",
    "        lock.acquire()\n",
    "        for word in local_index:\n",
    "            for doc_id in local_index[word]:\n",
    "                if (\n",
    "                    word not in pos_inverted_index\n",
    "                    or doc_id not in pos_inverted_index[word]\n",
    "                ):\n",
    "                    pos_inverted_index[word][doc_id] = []\n",
    "                pos_inverted_index[word][doc_id] += local_index[word][doc_id]\n",
    "    except:\n",
    "        print(\"Error processing batch\")\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "    finally:\n",
    "        lock.release()\n",
    "\n",
    "\n",
    "def positional_inverted_index(\n",
    "    file_name: str,\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    "    escape_char: bool = False,\n",
    "    headline: bool = True,\n",
    ") -> dict:\n",
    "    assert os.path.exists(\n",
    "        os.path.join(CURRENT_DIR, file_name)\n",
    "    ), f\"File {file_name} does not exist\"\n",
    "    xml_text = read_file(file_name)\n",
    "    doc_ids_set = set()\n",
    "    soup = BeautifulSoup(xml_text, \"html.parser\")\n",
    "    docs = soup.find_all(\"doc\")\n",
    "    doc_nos = soup.find_all(\"docno\")\n",
    "    for doc_no in doc_nos:\n",
    "        doc_ids_set.add(doc_no.text)\n",
    "    document_size = len(docs)\n",
    "    batch_size = document_size // NUM_OF_CORES\n",
    "    remainder = document_size % NUM_OF_CORES\n",
    "    pos_inverted_index = defaultdict(dict)\n",
    "    pos_inverted_index[\"document_size\"][\"0\"] = document_size\n",
    "    pos_inverted_index[\"doc_ids_list\"] = list(doc_ids_set)\n",
    "\n",
    "    batches = [docs[i * batch_size : (i + 1) * batch_size] for i in range(NUM_OF_CORES)]\n",
    "    if remainder != 0:\n",
    "        # append the remainder to the last batch\n",
    "        batches[-1] += docs[-remainder:]\n",
    "\n",
    "    threads = []\n",
    "    for batch in batches:\n",
    "        thread = threading.Thread(\n",
    "            target=process_batch,\n",
    "            args=(batch, pos_inverted_index, stopping, stemming, escape_char, headline),\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return pos_inverted_index\n",
    "\n",
    "\n",
    "# save as binary file\n",
    "def save_index_file(\n",
    "    file_name: str, index: defaultdict(dict), output_dir: str = \"binary_file\"\n",
    "):\n",
    "    if not os.path.exists(os.path.join(CURRENT_DIR, output_dir)):\n",
    "        os.mkdir(os.path.join(CURRENT_DIR, output_dir))\n",
    "    # sort index by term and doc_id in int\n",
    "    index_output = dict(sorted(index.items()))\n",
    "    for term, record in index_output.items():\n",
    "        if term == \"document_size\" or term == \"doc_ids_list\":\n",
    "            continue\n",
    "        index_output[term] = dict(sorted(record.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, file_name), \"wb\") as f:\n",
    "        for term, record in index_output.items():\n",
    "            if term == \"document_size\" or term == \"doc_ids_list\":\n",
    "                continue\n",
    "            f.write(f\"{term} {len(record)}\\n\".encode(\"utf8\"))\n",
    "            for doc_id, positions in record.items():\n",
    "                f.write(\n",
    "                    f\"\\t{doc_id}: {','.join([str(pos) for pos in positions])}\\n\".encode(\n",
    "                        \"utf8\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "def load_binary_index(file_name: str, output_dir: str = \"binary_file\") -> dict:\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, file_name), \"rb\") as f:\n",
    "        data = f.read().decode(\"utf8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "\n",
    "def load_queries(file_name: str) -> list:\n",
    "    query_lines = read_file(file_name).split(\"\\n\")\n",
    "    queries = []\n",
    "    for line in query_lines:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        query_id, query_text = line.split(\":\")\n",
    "        queries.append(query_text.strip())\n",
    "    return queries\n",
    "\n",
    "\n",
    "def handle_binary_operator(operator: str, left: list, right: list) -> list:\n",
    "    # print(\"handle binary operator\", operator, left, right)\n",
    "    left = [] if left is None else left\n",
    "    right = [] if right is None else right\n",
    "    if operator == \"AND\":\n",
    "        return list(set(left) & set(right))\n",
    "    elif operator == \"OR\":\n",
    "        return list(set(left) | set(right))\n",
    "\n",
    "\n",
    "def get_doc_ids_from_string(\n",
    "    string: str, inverted_index: dict, doc_ids_list: list, negate: bool = False\n",
    ") -> list:\n",
    "    # check if string is a phrase bounded by double quotes\n",
    "    if string in inverted_index:\n",
    "        if negate:\n",
    "            return negate_doc_ids(list(inverted_index[string].keys()), doc_ids_list)\n",
    "        else:\n",
    "            return list(inverted_index[string].keys()) if inverted_index[string] else []\n",
    "\n",
    "\n",
    "def get_doc_ids_from_pattern(\n",
    "    pattern: str, inverted_index: dict, doc_ids_list: list, negate: bool = False\n",
    "):\n",
    "    # pattern is of the form \"A B\"/\"A B C\" etc\n",
    "    # retrieve words from the pattern\n",
    "    doc_ids = []\n",
    "    words = re.findall(r\"\\w+\", pattern)\n",
    "    # check if the word are in consecutive positions\n",
    "    for doc_id in inverted_index[words[0]]:\n",
    "        positions = inverted_index[words[0]][doc_id]\n",
    "        for pos in positions:\n",
    "            try:\n",
    "                if (\n",
    "                    all(\n",
    "                        [\n",
    "                            pos + i in inverted_index[words[i]][doc_id]\n",
    "                            for i in range(1, len(words))\n",
    "                        ]\n",
    "                    )\n",
    "                    and doc_id not in doc_ids\n",
    "                ):\n",
    "                    doc_ids.append(doc_id)\n",
    "            except:\n",
    "                pass\n",
    "    if negate:\n",
    "        return negate_doc_ids(doc_ids, doc_ids_list)\n",
    "    else:\n",
    "        return doc_ids\n",
    "\n",
    "\n",
    "def negate_doc_ids(doc_ids: list, doc_ids_list: list) -> list:\n",
    "    return list(set(doc_ids_list) - set(doc_ids))\n",
    "\n",
    "\n",
    "def evaluate_proximity_pattern(\n",
    "    n: int, w1: str, w2: str, doc_ids_list: list, inverted_index: dict\n",
    ") -> list:\n",
    "    # find all the doc_ids for w1 and w2\n",
    "    doc_ids_for_w1 = get_doc_ids_from_string(w1, inverted_index, doc_ids_list)\n",
    "    # find the doc_ids that satisfy the condition\n",
    "    doc_ids = []\n",
    "    for doc_id in doc_ids_for_w1:\n",
    "        try:\n",
    "            positions_for_w1 = inverted_index[w1][doc_id]\n",
    "            positions_for_w2 = inverted_index[w2][doc_id]\n",
    "            if any(\n",
    "                [\n",
    "                    abs(pos1 - pos2) <= int(n)\n",
    "                    for pos1 in positions_for_w1\n",
    "                    for pos2 in positions_for_w2\n",
    "                ]\n",
    "            ):\n",
    "                doc_ids.append(doc_id)\n",
    "        except:\n",
    "            pass\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "def evaluate_subquery(\n",
    "    subquery: str,\n",
    "    inverted_index: dict,\n",
    "    doc_ids_list: list,\n",
    "    proximity_pattern: re.Pattern,\n",
    ") -> list:\n",
    "\n",
    "    proximity_match = re.match(proximity_pattern, subquery)\n",
    "    print(\"subquery\", subquery)\n",
    "    if proximity_match:\n",
    "        n = proximity_match.group(1)\n",
    "        w1 = proximity_match.group(2)\n",
    "        w2 = proximity_match.group(3)\n",
    "        print(\"Handle proximity pattern\", n, w1, w2)\n",
    "        return evaluate_proximity_pattern(n, w1, w2, doc_ids_list, inverted_index)\n",
    "    else:\n",
    "        # this must be a word or a phrase bounded by double quotes\n",
    "        negate_pattern = re.compile(r\"NOT (\\\"\\s+\\\")|NOT (\\w+)\")\n",
    "        match = re.match(negate_pattern, subquery)\n",
    "        if match:\n",
    "            # there is a NOT operator\n",
    "            negate = False if not match else True\n",
    "            if match.group(1):\n",
    "                string = match.group(1)\n",
    "                if string[0] == '\"' and string[-1] == '\"':\n",
    "                    string = string[1:-1]\n",
    "                print(\"handle phrase\", string, negate)\n",
    "                return get_doc_ids_from_pattern(\n",
    "                    string, inverted_index, doc_ids_list, negate\n",
    "                )\n",
    "            elif match.group(2):\n",
    "                string = match.group(2)\n",
    "                print(\"handle word\", string, negate)\n",
    "                result = get_doc_ids_from_string(\n",
    "                    string, inverted_index, doc_ids_list, negate\n",
    "                )\n",
    "                return result\n",
    "        else:\n",
    "            # there is no NOT operator\n",
    "            if subquery[0] == '\"' and subquery[-1] == '\"':\n",
    "                print(\"handle phrase\", subquery[1:-1])\n",
    "                return get_doc_ids_from_pattern(\n",
    "                    subquery[1:-1], inverted_index, doc_ids_list\n",
    "                )\n",
    "            else:\n",
    "                print(\"handle word\", subquery)\n",
    "                return get_doc_ids_from_string(subquery, inverted_index, doc_ids_list)\n",
    "\n",
    "\n",
    "def read_boolean_queries(file_name: str) -> list:\n",
    "    queries = []\n",
    "    with open(os.path.join(CURRENT_DIR, file_name), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            # split the query by the first space\n",
    "            query_id, query = line.split(\" \", 1)\n",
    "            queries.append((query_id, query.strip()))\n",
    "    return queries\n",
    "\n",
    "\n",
    "def read_ranked_queries(file_name: str) -> list:\n",
    "    queries = []\n",
    "    with open(os.path.join(CURRENT_DIR, file_name), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            # split the query by the first space\n",
    "            query_id, query = line.split(\" \", 1)\n",
    "            queries.append((query_id, query.strip()))\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def calculate_tf_idf(\n",
    "    inverted_index: dict, tokens: list, doc_id: str, docs_size: int\n",
    ") -> float:\n",
    "    tf_idf_score = 0\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index or doc_id not in inverted_index[token]:\n",
    "            continue\n",
    "        tf = 1 + math.log10(len(inverted_index[token][doc_id]))\n",
    "        idf = math.log10(docs_size / len(inverted_index[token]))\n",
    "        tf_idf_score += tf * idf\n",
    "    return tf_idf_score\n",
    "\n",
    "\n",
    "def evaluate_boolean_query(\n",
    "    query: str,\n",
    "    inverted_index: dict,\n",
    "    doc_ids_list: list,\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    ") -> list:\n",
    "    # Rule 1: all words are of length 1 unless bounded by double quotes\n",
    "    # Rule 2: No brackets unless #n() is used\n",
    "    # Rule 3. NOT Operator is used as a unary operator\n",
    "    # Rule 4: AND and OR are binary operators\n",
    "    # Rule 5: #n() is binary operator i.e. #n(w1, w2)\n",
    "    # Rule 6: NOT should be followed by a word or a phrase bounded by double quotes\n",
    "    # Rule 7: Each query only contains one OR/AND operator i,e, split -> evaluate left side -> evaluate right side -> combine\n",
    "    # split the query by the operators and regular expressions\n",
    "    # operators: AND, OR\n",
    "    # regular expressions: #n(), \"word\"\n",
    "    # split the string by AND and OR while keeping the seperator\n",
    "    # lower case the query if it is not an operator\n",
    "    query = re.sub(r\"(\\w+)\", lambda x: preprocess_match(x, stopping, stemming), query)\n",
    "    query = \" \".join(\n",
    "        [\n",
    "            token.lower() if token not in [\"AND\", \"OR\", \"NOT\"] else token\n",
    "            for token in query.split(\" \")\n",
    "        ]\n",
    "    )\n",
    "    tokens = re.split(r\"\\s*(AND|OR)\\s*\", query)\n",
    "    # for token in tokens:\n",
    "    #     token = re.sub(r\"(\\w+)\", get_preprocessed_word, token)\n",
    "\n",
    "    proximity_pattern = re.compile(r\"#(\\d+)\\((\\w+),\\s*(\\w+)\\)\")\n",
    "    # lower case the tokens if they are not operators\n",
    "\n",
    "    print(\"tokens\", tokens)\n",
    "\n",
    "    try:\n",
    "        # the length of tokens must be 3 or 1\n",
    "        if len(tokens) == 1:\n",
    "            return evaluate_subquery(\n",
    "                tokens[0], inverted_index, doc_ids_list, proximity_pattern\n",
    "            )\n",
    "        else:\n",
    "            # there must be an operator\n",
    "            operator = tokens[1]\n",
    "            left = evaluate_subquery(\n",
    "                tokens[0], inverted_index, doc_ids_list, proximity_pattern\n",
    "            )\n",
    "            right = evaluate_subquery(\n",
    "                tokens[2], inverted_index, doc_ids_list, proximity_pattern\n",
    "            )\n",
    "            return handle_binary_operator(operator, left, right)\n",
    "\n",
    "    except:\n",
    "        # print the processing error term\n",
    "        print(tokens)\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "\n",
    "\n",
    "def evaluate_ranked_query(\n",
    "    queries: list,\n",
    "    index: defaultdict,\n",
    "    max_result: int = 150,\n",
    "    stopping: bool = True,\n",
    "    stemming: bool = True,\n",
    ") -> list:\n",
    "    results = []\n",
    "    docs_size = int(index[\"document_size\"][\"0\"])\n",
    "    for query_id, query in queries:\n",
    "        words = get_preprocessed_words(query, stopping, stemming)\n",
    "        print(words)\n",
    "        doc_ids = set()\n",
    "        for word in words:\n",
    "            if word in index:\n",
    "                doc_ids = doc_ids.union(set(index[word].keys()))\n",
    "\n",
    "        doc_ids = list(doc_ids)\n",
    "        scores = []\n",
    "        for doc_id in doc_ids:\n",
    "            scores.append((doc_id, calculate_tf_idf(index, words, doc_id, docs_size)))\n",
    "        # sort by the score and the doc_id\n",
    "        scores.sort(key=lambda x: (-x[1], x[0]))\n",
    "        results.append((query_id, scores[:max_result]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_boolean_queries_result(results: list, output_dir: str = \"result\"):\n",
    "    if not os.path.exists(os.path.join(CURRENT_DIR, output_dir)):\n",
    "        os.mkdir(os.path.join(CURRENT_DIR, output_dir))\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, \"results.boolean.txt\"), \"w\") as f:\n",
    "        for query_id, result in results:\n",
    "            for doc_id in result:\n",
    "                f.write(f\"{query_id},{doc_id}\\n\")\n",
    "\n",
    "\n",
    "def save_ranked_queries_result(results: list, output_dir: str = \"result\"):\n",
    "    if not os.path.exists(os.path.join(CURRENT_DIR, output_dir)):\n",
    "        os.mkdir(os.path.join(CURRENT_DIR, output_dir))\n",
    "    with open(os.path.join(CURRENT_DIR, output_dir, \"results.ranked.txt\"), \"w\") as f:\n",
    "        for query_id, result in results:\n",
    "            for retrieved_doc_result in result:\n",
    "                doc_id, score = retrieved_doc_result\n",
    "                f.write(f\"{query_id},{doc_id},{score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to build index 40.9874267578125\n",
      "Time taken to load index 1.5790095329284668\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Asus\\\\Desktop\\\\ttds-proj\\\\backend\\\\utils\\\\index building patrikas\\\\queries.boolean.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken to load index\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m### processing boolean queries\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m boolean_queries \u001b[38;5;241m=\u001b[39m read_boolean_queries(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries.boolean.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m boolean_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m doc_ids_list \u001b[38;5;241m=\u001b[39m index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_ids_list\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 370\u001b[0m, in \u001b[0;36mread_boolean_queries\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_boolean_queries\u001b[39m(file_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    369\u001b[0m     queries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CURRENT_DIR, file_name), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;66;03m# split the query by the first space\u001b[39;00m\n\u001b[0;32m    373\u001b[0m             query_id, query \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Asus\\\\Desktop\\\\ttds-proj\\\\backend\\\\utils\\\\index building patrikas\\\\queries.boolean.txt'"
     ]
    }
   ],
   "source": [
    "output_dir = \"\"\n",
    "custom_index_dir = 'index'\n",
    "start = time.time()\n",
    "index = positional_inverted_index(XML_FILES[2], stopping=True, stemming=True, escape_char=False, headline=True)\n",
    "print(\"Time taken to build index\", time.time() - start)\n",
    "# save the index file which specifies the required format for submission\n",
    "save_index_file(\"index.txt\", index, '')\n",
    "# save the custom index file\n",
    "save_json_file(\"index.json\", index, custom_index_dir)\n",
    "\n",
    "### loading index\n",
    "start = time.time()\n",
    "index = load_binary_index(\"index.json\", custom_index_dir)\n",
    "print(\"Time taken to load index\", time.time() - start)\n",
    "\n",
    "### processing boolean queries\n",
    "boolean_queries = read_boolean_queries(\"queries.boolean.txt\")\n",
    "boolean_results = []\n",
    "doc_ids_list = index['doc_ids_list']\n",
    "start = time.time()\n",
    "for query in boolean_queries:\n",
    "    query_id, query_text = query\n",
    "    retrieved_docs = evaluate_boolean_query(query_text, index, doc_ids_list)\n",
    "    retrieved_docs = [int(x) for x in retrieved_docs]\n",
    "    retrieved_docs.sort()\n",
    "    boolean_results.append((query_id, retrieved_docs))\n",
    "\n",
    "save_boolean_queries_result(boolean_results, '')\n",
    "print(\"Time taken to process boolean queries\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_queries = read_ranked_queries(\"queries.ranked.txt\")\n",
    "start = time.time()\n",
    "ranked_results = evaluate_ranked_query(ranked_queries, index)\n",
    "save_ranked_queries_result(ranked_results, '')\n",
    "print(\"Time taken to process ranked queries\", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
