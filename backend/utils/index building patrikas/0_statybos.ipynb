{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict, Counter\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, SnowballStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "ENGLISH_STOPWORDS = open(\"english_stop_words.txt\").read().split(\"\\n\")\n",
    "\n",
    "# from https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n",
    "\"\"\"\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "# re.compile object to help to find document places with contractions\n",
    "CONTRACTIONS_PATTERN = re.compile(\n",
    "    \"({})\".format(\"|\".join(CONTRACTION_MAP.keys())),\n",
    "    flags=re.IGNORECASE | re.DOTALL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_parse_XML(xml_path: str)->tuple:\n",
    "    \"\"\"\n",
    "    Load and parse an XML file to extract documents and headlines.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    xml_path : str\n",
    "        The file path to the XML file that needs to be loaded and parsed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    documents_dict : dict\n",
    "        A dictionary where the keys are document IDs (either integers or strings)\n",
    "        and the values are the full text of each document (concatenation of headline and text).\n",
    "\n",
    "    id_to_headline_dict : dict\n",
    "        A dictionary mapping document IDs (either integers or strings) to their corresponding headlines.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function uses the ElementTree (ET) library to parse the XML data.\n",
    "    - Both the headline and text are stripped of leading/trailing whitespace and concatenated to form the full text.\n",
    "    - In case the document ID cannot be converted to an integer, a warning is printed.\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> documents_dict, id_to_headline_dict = load_and_parse_XML(\"path/to/xml/file\")\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the documents\n",
    "    documents_dict = {}\n",
    "    # Initialize an empty dictionary to map integers to headlines\n",
    "    id_to_headline_dict = {}\n",
    "\n",
    "    # Load and parse the XML data from the file\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Loop through each DOC element in the XML tree\n",
    "    for doc in root.findall(\"DOC\"):\n",
    "        # Get the DOCNO element's text, removing any extra whitespace\n",
    "        doc_id = doc.find(\"DOCNO\").text.strip()\n",
    "\n",
    "        # Convert the DOCNO to an integer, if possible\n",
    "        try:\n",
    "            doc_id = int(doc_id)\n",
    "        except ValueError:\n",
    "            # Handle the case where DOCNO is not convertible to an integer\n",
    "            print(f\"Warning: Could not convert DOCNO '{doc_id}' to an integer.\")\n",
    "        # Extract the headline and text, and replace '\\n' with a space\n",
    "        headline = (\n",
    "            doc.find(\"HEADLINE\").text.strip().replace(\"\\n\", \" \").replace(\"'\", \"'\")\n",
    "            if doc.find(\"HEADLINE\") is not None\n",
    "            else \"\"\n",
    "        )\n",
    "        text = (\n",
    "            doc.find(\"TEXT\").text.strip().replace(\"\\n\", \" \").replace(\"'\", \"'\")\n",
    "            if doc.find(\"TEXT\") is not None\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "        # Concatenate the headline and text, replacing '\\n' with a space\n",
    "        full_text = f\"{headline} {text}\"  # Used space to concatenate\n",
    "\n",
    "        # Store the full text in the dictionary, using the doc_id as the key\n",
    "        documents_dict[doc_id] = full_text\n",
    "        # Map the doc_id to the headline\n",
    "        id_to_headline_dict[doc_id] = headline\n",
    "    return documents_dict, id_to_headline_dict\n",
    "\n",
    "\n",
    "def expand_match(contraction) -> str:\n",
    "    \"\"\"\n",
    "    Expand a specific matched contraction within a text string.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    contraction : re.Match object\n",
    "        The re.Match object containing the contraction to be expanded.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The expanded version of the matched contraction.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function looks up the contraction from a pre-defined map (CONTRACTION_MAP) to find its expanded form.\n",
    "    - The case of the first letter is preserved in the expansion.\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> expand_match(re.match(r\"\\bI'm\\b\", \"I'm fine\"))\n",
    "    \"I am\"\n",
    "    \"\"\"\n",
    "    # re.Match object, group(0) is the match str\n",
    "    match = contraction.group(0)\n",
    "    # in case it's lowercase\n",
    "    first_char = match[0]\n",
    "    # find the matching key and value from the map\n",
    "    expanded_contraction = (\n",
    "        CONTRACTION_MAP.get(match)\n",
    "        if CONTRACTION_MAP.get(match)\n",
    "        else CONTRACTION_MAP.get(match.lower())\n",
    "    )\n",
    "    expanded_contraction = first_char + expanded_contraction[1:]\n",
    "    return expanded_contraction\n",
    "\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand all contractions in a given text string.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The text string containing contractions to be expanded.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The text string with all contractions expanded and apostrophes removed.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - This function internally calls `expand_match` for each contraction found.\n",
    "    - All remaining apostrophes in the text are replaced with spaces.\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> expand_contractions(\"I'm fine, aren't I?\")\n",
    "    \"I am fine, are not I \"\n",
    "    \"\"\"\n",
    "    expanded_text = CONTRACTIONS_PATTERN.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \" \", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def get_processed_tokens_from_string(\n",
    "    document: str, contraction_expansion: bool = True, stopword_removal: bool = True\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Tokenize and preprocess a given text document.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    document : str\n",
    "        The text document to be tokenized and preprocessed.\n",
    "        \n",
    "    contraction_expansion : bool, optional (default=True)\n",
    "        Whether to expand contractions in the text document.\n",
    "        \n",
    "    stopword_removal : bool, optional (default=True)\n",
    "        Whether to remove stopwords from the tokenized text.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        A list of processed tokens from the document.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - Tokenization is performed using the `word_tokenize` function from the nltk library.\n",
    "    - Text is converted to lowercase during processing.\n",
    "    - Non-alphabetic characters are removed from the document.\n",
    "    - If enabled, contractions are expanded using the `expand_contractions` function.\n",
    "    - If enabled, stopwords are removed using a predefined list (ENGLISH_STOPWORDS).\n",
    "    - Stemming is performed on each token using the Porter stemmer.\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> get_processed_tokens_from_string(\"I'm going to the store.\")\n",
    "    ['go', 'store']\n",
    "\n",
    "    >>> get_processed_tokens_from_string(\"I'm going to the store.\", stopword_removal=False)\n",
    "    ['i', 'am', 'go', 'to', 'the', 'store']\n",
    "    \"\"\"\n",
    "    if contraction_expansion:\n",
    "        document = expand_contractions(document)\n",
    "    document = re.sub(r\"[^a-zA-Z\\s]\", \" \", document).replace(\"[\", \" \").replace(\"]\", \" \")\n",
    "    tokens = word_tokenize(document)\n",
    "    # to lowercase\n",
    "    tokens = [w.lower() for w in tokens]  # must be list, not set\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        tokens = list(filter(lambda token: token not in ENGLISH_STOPWORDS, tokens))\n",
    "    # finally stem with Porter stemmer, by each TOKEN\n",
    "    tokens = [STEMMER.stem(term) for term in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_index_corpus(\n",
    "    document_dictionary: dict,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Preprocesses the documents and constructs an inverted index corpus.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    document_dictionary : dict\n",
    "        Dictionary of documents to index. The keys are identifiers (article docNOs)\n",
    "        and the values are the corresponding text articles.\n",
    "\n",
    "    contraction_expansion : bool, optional (default=True)\n",
    "        Whether to expand contractions in the text during preprocessing.\n",
    "\n",
    "    stopword_removal : bool, optional (default=True)\n",
    "        Whether to remove stopwords from the text during preprocessing.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        An inverted index dictionary. Each key is a unique term, and the value is a dictionary\n",
    "        containing the Document Frequency and a list of Postings (document IDs and term positions).\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The inverted index is sorted first by term and then by document ID.\n",
    "    - The Document Frequency for each term is calculated.\n",
    "    - The construction time for the index is printed.\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> create_index_corpus({'doc1': 'apple orange', 'doc2': 'orange banana'})\n",
    "    {'apple': {'Document Frequency': 1, 'Postings': {'doc1': [0]}},\n",
    "     'banana': {'Document Frequency': 1, 'Postings': {'doc2': [1]}},\n",
    "     'orange': {'Document Frequency': 2, 'Postings': {'doc1': [1], 'doc2': [0]}}}\n",
    "    \"\"\"\n",
    "    print(\"\\nPreprocessing documents:\")\n",
    "    index_construction_start = time.time()\n",
    "    # preprocess each string of an article\n",
    "    for article in tqdm(document_dictionary):\n",
    "        document_dictionary[article] = get_processed_tokens_from_string(\n",
    "            document_dictionary[article], contraction_expansion, stopword_removal\n",
    "        )\n",
    "    token_sequence = []\n",
    "    # Initialize a set for each term:\n",
    "    term_doc_set = defaultdict(set)  # Step 1: Initialize a set for each term\n",
    "\n",
    "    # for each article string\n",
    "    for article in document_dictionary:\n",
    "        # get the index of the term and the term\n",
    "        for index, term in enumerate(document_dictionary[article]):\n",
    "            # append the term, the episode, and the location number of that term/multi-term in an article\n",
    "            # to a list of such entries for the whole corpus\n",
    "            term_article_tuple = (\n",
    "                term,\n",
    "                article,\n",
    "                index,  # ...so we substract the number of them we had previously in the document...\n",
    "            )\n",
    "            token_sequence.append(term_article_tuple)\n",
    "            term_doc_set[term].add(article)  # Add the docID to the set\n",
    "    print(\"\\nSorting index...\")\n",
    "    index_sorting_start = time.time()\n",
    "    # Sort first by term (alphabetically), then by docID\n",
    "    token_sequence = sorted(\n",
    "        token_sequence,\n",
    "        key=lambda x: (x[0], x[1]),\n",
    "    )\n",
    "    print(\n",
    "        \"Index sorting took {0:4.2f} seconds.\".format(\n",
    "            (time.time() - index_sorting_start)\n",
    "        )\n",
    "    )\n",
    "    inverted_index_dictionary = defaultdict(\n",
    "        lambda: {\"Document Frequency\": 0, \"Postings\": defaultdict(list)}\n",
    "    )\n",
    "    # Construct the inverted index\n",
    "    for entry in token_sequence:\n",
    "        term, doc_id, location = entry\n",
    "        inverted_index_dictionary[term][\"Postings\"][doc_id].append(location)\n",
    "    # Set the frequency to the size of the set of docIDs\n",
    "    for term in inverted_index_dictionary.keys():\n",
    "        inverted_index_dictionary[term][\"Document Frequency\"] = len(term_doc_set[term])\n",
    "    print(\n",
    "        \"Final index has {0} terms and took {1:4.2f} seconds to construct.\".format(\n",
    "            len(inverted_index_dictionary.keys()),\n",
    "            time.time() - index_construction_start,\n",
    "        )\n",
    "    )\n",
    "    return inverted_index_dictionary\n",
    "\n",
    "\n",
    "def output_inverted_index_to_txt(inverted_index: dict, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Outputs the inverted index data to a text file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    inverted_index : dict\n",
    "        The inverted index dictionary to be written to file.\n",
    "\n",
    "    output_path : str\n",
    "        The path where the text file will be saved.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for term, term_data in inverted_index.items():\n",
    "            # Write the term and its document frequency\n",
    "            f.write(f\"{term}:{term_data['Document Frequency']}\\n\")\n",
    "            for doc_id, positions in term_data[\"Postings\"].items():\n",
    "                # Write the document ID and positions\n",
    "                f.write(f\"   {doc_id}: {','.join(map(str, positions))}\\n\")\n",
    "\n",
    "\n",
    "def create_and_save_index(\n",
    "    xml_path: str,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    "    csv: bool = True,\n",
    "    txt: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates and saves an inverted index from XML data to multiple file formats.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    xml_path : str\n",
    "        The path to the XML file containing the documents to index.\n",
    "\n",
    "    contraction_expansion : bool, optional (default=True)\n",
    "        Whether to expand contractions in the text during preprocessing.\n",
    "\n",
    "    stopword_removal : bool, optional (default=True)\n",
    "        Whether to remove stopwords from the text during preprocessing.\n",
    "\n",
    "    csv : bool, optional (default=True)\n",
    "        Whether to save the index to a CSV file.\n",
    "\n",
    "    txt : bool, optional (default=True)\n",
    "        Whether to save the index to a text file.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - Index and additional dictionary to match doc IDs to headlines are saved as pickled files.\n",
    "    \"\"\"\n",
    "    document_dictionary, id_to_headline_dict = load_and_parse_XML(xml_path=xml_path)\n",
    "    inverted_index_dictionary = create_index_corpus(\n",
    "        document_dictionary=document_dictionary,\n",
    "        contraction_expansion=contraction_expansion,\n",
    "        stopword_removal=stopword_removal,\n",
    "    )\n",
    "    with open(\"inverted_index.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(inverted_index_dictionary), f)\n",
    "        print(\"Inverted index saved to file 'inverted_index.pkl'.\")\n",
    "    with open(\"id_to_headline_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(id_to_headline_dict, f)\n",
    "        print(\"ID to headline dictionary saved to file 'id_to_headline_dict.pkl'.\")\n",
    "    if csv:\n",
    "        df = pd.DataFrame.from_dict(inverted_index_dictionary, orient=\"index\")\n",
    "        df.to_csv(\"inverted_index.csv\")\n",
    "        print(\"Inverted index saved to file 'inverted_index.csv'.\")\n",
    "    if txt:\n",
    "        output_inverted_index_to_txt(inverted_index_dictionary, \"inverted_index.txt\")\n",
    "        print(\"Inverted index saved to file 'inverted_index.txt'.\")\n",
    "\n",
    "\n",
    "def load_pickled_data(\n",
    "    inverted_index_path: str = \"inverted_index.pkl\",\n",
    "    id_to_headline_path=\"id_to_headline_dict.pkl\",\n",
    ")->tuple:\n",
    "    \"\"\"\n",
    "    Loads pickled inverted index and ID-to-headline dictionary data from disk.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    inverted_index_path : str, optional (default=\"inverted_index.pkl\")\n",
    "        Path to the pickled file containing the inverted index.\n",
    "\n",
    "    id_to_headline_path : str, optional (default=\"id_to_headline_dict.pkl\")\n",
    "        Path to the pickled file containing the ID-to-headline dictionary.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the loaded inverted index dictionary and the ID-to-headline dictionary.\n",
    "    \"\"\"\n",
    "    with open(inverted_index_path, \"rb\") as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "    inverted_index_dict = defaultdict(\n",
    "        lambda: {\"Document Frequency\": 0, \"Postings\": defaultdict(list)}, loaded_dict\n",
    "    )\n",
    "\n",
    "    with open(id_to_headline_path, \"rb\") as f:\n",
    "        id_to_headline_dict = pickle.load(f)\n",
    "    return inverted_index_dict, id_to_headline_dict\n",
    "\n",
    "\n",
    "def parse_unranked_query(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Parses a single unranked boolean query and identifies its type.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query : str\n",
    "        The query string to parse.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        A list containing the type of the query and its components. For example:\n",
    "        - (\"AND\", [\"Edinburgh\", \"SCOTLAND\"]) for \"Edinburgh AND SCOTLAND\"\n",
    "        - (\"PHRASE\", \"income taxes\") for \"\\\"income taxes\\\"\"\n",
    "        - (\"PROXIMITY\", ([\"income\", \"taxes\"], 20)) for \"#20(income, taxes)\"\n",
    "        - (\"TERM\", \"Happiness\") for \"Happiness\"\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - Supported query types are: AND, OR, NOT, PHRASE, PROXIMITY, TERM\n",
    "    \"\"\"\n",
    "    # Identify the type of query: AND, OR, NOT, phrase, or proximity\n",
    "    if \" AND NOT \" in query:\n",
    "        return \"AND NOT\", query.split(\" AND NOT \")\n",
    "    elif \" OR NOT \" in query:\n",
    "        return \"OR NOT\", query.split(\" OR NOT \")\n",
    "    elif \" AND \" in query:\n",
    "        return \"AND\", query.split(\" AND \")\n",
    "    elif \" OR \" in query:\n",
    "        return \"OR\", query.split(\" OR \")\n",
    "    elif query.startswith('\"') and query.endswith('\"'):\n",
    "        return \"PHRASE\", query[1:-1]\n",
    "    elif re.match(r\"\\s*#\\d+\\(.*\\)\", query):\n",
    "        match = re.match(r\"\\s*#(\\d+)\\((.*),(.*)\\)\", query)\n",
    "        distance = int(match[1])\n",
    "        terms = match[2].strip(), match[3].strip()\n",
    "        return \"PROXIMITY\", (terms, distance)\n",
    "    else:\n",
    "        if query != \"\":\n",
    "            return \"TERM\", query\n",
    "\n",
    "\n",
    "def get_boolean_query_list(query_file_path: str, chars_to_skip: int = 2) -> list:\n",
    "    \"\"\"\n",
    "    Parses a text file containing boolean queries and returns them as a list.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    query_file_path : str\n",
    "        The path to the text file containing boolean queries.\n",
    "\n",
    "    chars_to_skip : int, optional (default=2)\n",
    "        The number of characters to skip at the beginning of each line in the query file.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        A list containing parsed boolean queries. Each element is the output from `parse_unranked_query`.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - Each line in the text file should contain a single boolean query.\n",
    "    - The function will remove None values from the list.\n",
    "\n",
    "    Example query file content:\n",
    "    ---------------------------\n",
    "    1 Happiness\n",
    "    2 Edinburgh AND SCOTLAND\n",
    "    ...\n",
    "    \"\"\"\n",
    "    queries = open(query_file_path).read().split(\"\\n\")\n",
    "    query_list = [\n",
    "        parse_unranked_query(queries[idx][chars_to_skip:])\n",
    "        for idx, element in enumerate(queries)\n",
    "    ]\n",
    "    return [item for item in query_list if item is not None]\n",
    "\n",
    "\n",
    "def execute_single_term_search(\n",
    "    term: str,\n",
    "    inverted_index: dict,\n",
    "    verbose: bool = False,\n",
    "    stopword_removal: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Executes a single-term search on an inverted index and returns the document IDs containing the term.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    term : str\n",
    "        The search term. This term will be processed before searching in the inverted index.\n",
    "\n",
    "    inverted_index : dict\n",
    "        The inverted index to search.\n",
    "\n",
    "    verbose : bool, optional (default=False)\n",
    "        If set to True, prints additional information about the search.\n",
    "\n",
    "    stopword_removal : bool, optional (default=True)\n",
    "        If set to True, stopwords are removed from the search term.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        A list of document IDs where the term appears.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    AssertionError\n",
    "        If the processed term contains other than 1 term, an AssertionError is raised.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The term will be preprocessed (e.g., stopwords removed, stemmed) before being searched in the index.\n",
    "    - This function only supports single-term queries.\n",
    "    \"\"\"\n",
    "    term = get_processed_tokens_from_string(term, stopword_removal=stopword_removal)\n",
    "    assert (\n",
    "        len(term) == 1\n",
    "    ), f\"Term search must contain exactly 1 term. Found {len(term)} term(s) instead.\"\n",
    "    if verbose:\n",
    "        print(\"Executing single term search on processed term '\" + term[0] + \"'.\\n\")\n",
    "    return list((inverted_index[((term)[0])][\"Postings\"]).keys())\n",
    "\n",
    "\n",
    "def execute_proximity_search(\n",
    "    term_1: str,\n",
    "    term_2: str,\n",
    "    distance: int,\n",
    "    inverted_index: dict,\n",
    "    strictly_ordering_tokens: bool,\n",
    "    verbose: bool = False,\n",
    "    full_output: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a proximity search for two terms in an inverted index and returns the document IDs\n",
    "    that satisfy the proximity constraint.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    term_1 : str\n",
    "        The first search term.\n",
    "\n",
    "    term_2 : str\n",
    "        The second search term.\n",
    "\n",
    "    distance : int\n",
    "        The maximum allowable distance between the two terms.\n",
    "\n",
    "    inverted_index : dict\n",
    "        The inverted index in which to search.\n",
    "\n",
    "    strictly_ordering_tokens : bool\n",
    "        If True, maintains the order of tokens in proximity; otherwise, the order is not considered.\n",
    "\n",
    "    verbose : bool, optional (default=False)\n",
    "        If True, prints additional information during the operation.\n",
    "\n",
    "    full_output : bool, optional (default=False)\n",
    "        If True, returns a dictionary with detailed position information; otherwise, returns a list of doc_ids.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict or list\n",
    "        If full_output is True, returns a dictionary where keys are document IDs and values are lists\n",
    "        of tuple positions (term_1_position, term_2_position) (only used for debugging).\n",
    "        Otherwise, returns a sorted list of unique document IDs where the terms meet the proximity constraint.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    AssertionError\n",
    "        Raised if either of the terms does not condense to a single term after processing.\n",
    "    \"\"\"\n",
    "    # Define functions for distance comparison\n",
    "    def ordered_distance(pos1, pos2, distance):\n",
    "        return 0 <= (pos2 - pos1) <= distance\n",
    "\n",
    "    def unordered_distance(pos1, pos2, distance):\n",
    "        return abs(pos1 - pos2) <= distance\n",
    "\n",
    "    # Choose 1 of the 2 distance functions\n",
    "    distance_fn = ordered_distance if strictly_ordering_tokens else unordered_distance\n",
    "\n",
    "    term_1 = get_processed_tokens_from_string(term_1)\n",
    "    term_2 = get_processed_tokens_from_string(term_2)\n",
    "\n",
    "    assert (\n",
    "        len(term_1) == 1 and len(term_2) == 1\n",
    "    ), f\"Term search must contain exactly 1 term. Found {len(term_1)} {len(term_2)} term(s) instead.\"\n",
    "\n",
    "    # Get the actual terms from the lists\n",
    "    term_1, term_2 = term_1[0], term_2[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Executing proximity search on processed terms '\"\n",
    "            + str(term_1)\n",
    "            + \"' and '\"\n",
    "            + str(term_2)\n",
    "            + \"' with distance '\"\n",
    "            + str(distance)\n",
    "            + \" and strict_ordering = '\"\n",
    "            + str(strictly_ordering_tokens)\n",
    "            + \"'.\\n\"\n",
    "        )\n",
    "    \n",
    "    # Sorted only for prettier output\n",
    "    doc_ids_of_mutual_occurance = sorted(\n",
    "        set(\n",
    "            inverted_index[term_1][\"Postings\"].keys()\n",
    "            & inverted_index[term_2][\"Postings\"].keys()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # If full_output is True, return detailed position information of\n",
    "    # both terms for each co-occurance satient doc_id\n",
    "    if full_output:\n",
    "        coexistence_dictionary = defaultdict(list)\n",
    "        for doc_id in doc_ids_of_mutual_occurance:\n",
    "            term_1_positions = inverted_index[term_1][\"Postings\"][doc_id]\n",
    "            term_2_positions = inverted_index[term_2][\"Postings\"][doc_id]\n",
    "\n",
    "            for term_1_position in term_1_positions:\n",
    "                for term_2_position in term_2_positions:\n",
    "                    if distance_fn(term_1_position, term_2_position, distance):\n",
    "                        coexistence_dictionary[doc_id].append(\n",
    "                            (term_1_position, term_2_position)\n",
    "                        )\n",
    "        return coexistence_dictionary\n",
    "    # If full_output is False, return only the doc_ids - again, sorted for prettier output\n",
    "    else:\n",
    "        coexistence_list = []\n",
    "        for doc_id in doc_ids_of_mutual_occurance:\n",
    "            term_1_positions = inverted_index[term_1][\"Postings\"][doc_id]\n",
    "            term_2_positions = inverted_index[term_2][\"Postings\"][doc_id]\n",
    "\n",
    "            for term_1_position in term_1_positions:\n",
    "                for term_2_position in term_2_positions:\n",
    "                    if distance_fn(term_1_position, term_2_position, distance):\n",
    "                        coexistence_list.append(doc_id)\n",
    "        return list(sorted(set(coexistence_list)))\n",
    "\n",
    "\n",
    "def execute_phrase_search(\n",
    "    phrase: str,\n",
    "    inverted_index: dict,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Executes a phrase search for a given phrase in an inverted index and returns the document IDs\n",
    "    where the phrase appears exactly as specified.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    phrase : str\n",
    "        The search phrase, consisting of two terms that appear consecutively in the document.\n",
    "\n",
    "    inverted_index : dict\n",
    "        The inverted index in which to search.\n",
    "\n",
    "    verbose : bool, optional (default=False)\n",
    "        If True, prints additional information during the operation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of document IDs where the phrase appears exactly as given.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    AssertionError\n",
    "        Raised if the phrase does not condense to exactly two terms after processing.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function uses execute_proximity_search() internally to find the documents where the terms\n",
    "      appear with a distance of 1 and in the specified order.\n",
    "\n",
    "    \"\"\"\n",
    "    terms = get_processed_tokens_from_string(phrase)\n",
    "    assert (\n",
    "        len(terms) == 2\n",
    "    ), f\"Term search must contain exactly 1 term. Found {len(terms)} term(s) instead.\"\n",
    "    return execute_proximity_search(\n",
    "        term_1=terms[0],\n",
    "        term_2=terms[1],\n",
    "        distance=1,\n",
    "        inverted_index=inverted_index,\n",
    "        strictly_ordering_tokens=True,\n",
    "        verbose=verbose,\n",
    "        full_output=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def execute_logical_search(\n",
    "    operator_name: str,\n",
    "    term_1: str,\n",
    "    term_2: str,\n",
    "    inverted_index: dict,\n",
    "    verbose: bool = False,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ") -> set:\n",
    "    \"\"\"\n",
    "    Executes a Boolean search operation between two terms or phrases in an inverted index.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    operator_name : str\n",
    "        The Boolean operator to use for the search. Must be one of the following: 'AND', 'OR', 'AND NOT', 'OR NOT'.\n",
    "\n",
    "    term_1, term_2 : str\n",
    "        The search terms or phrases.\n",
    "\n",
    "    inverted_index : dict\n",
    "        The inverted index in which to perform the search.\n",
    "\n",
    "    verbose : bool, optional (default=False)\n",
    "        If True, prints additional information during the operation.\n",
    "\n",
    "    contraction_expansion : bool, optional (default=True)\n",
    "        Whether or not to expand contractions in the search terms.\n",
    "\n",
    "    stopword_removal : bool, optional (default=True)\n",
    "        Whether or not to remove stopwords from the search terms.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    set\n",
    "        A sorted set of document IDs where the Boolean condition between the terms/phrases is met.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    Exception\n",
    "        Raised if the operator name is not valid or if the token number for any of the terms is incorrect.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The terms are preprocessed (e.g., stopwords removed, stemmed, contractions expanded) before the search.\n",
    "    - Both single terms and two-term phrases can be used for term_1 and term_2.\n",
    "    \"\"\"\n",
    "    # Determine operator and use operator module or lambda functions\n",
    "    if operator_name == \"AND\":\n",
    "        boolean_operator = operator.and_\n",
    "    elif operator_name == \"OR\":\n",
    "        boolean_operator = operator.or_\n",
    "    elif operator_name == \"AND NOT\":\n",
    "        boolean_operator = lambda a, b: a - b\n",
    "    elif operator_name == \"OR NOT\":\n",
    "        boolean_operator = lambda a, b: a.union(b) - a.intersection(b)\n",
    "    else:\n",
    "        raise Exception(\"Operator name is not valid\")\n",
    "    \n",
    "    term_1 = get_processed_tokens_from_string(\n",
    "        term_1, contraction_expansion, stopword_removal\n",
    "    )\n",
    "    term_2 = get_processed_tokens_from_string(\n",
    "        term_2, contraction_expansion, stopword_removal\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Executing \"\n",
    "            + operator_name\n",
    "            + \" search on processed terms '\"\n",
    "            + str(term_1)\n",
    "            + \"', '\"\n",
    "            + str(term_2)\n",
    "            + \"'.\\n\"\n",
    "        )\n",
    "\n",
    "    # Handle different combinations of single terms and phrases\n",
    "    if len(term_1) == 1 and len(term_2) == 1:\n",
    "        # Both are single words - sorting only for prettier output\n",
    "        return list(\n",
    "            sorted(\n",
    "                boolean_operator(\n",
    "                    set(\n",
    "                        execute_single_term_search(\n",
    "                            term=term_1[0],\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                    set(\n",
    "                        execute_single_term_search(\n",
    "                            term=term_2[0],\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    elif len(term_1) == 2 and len(term_2) == 1:\n",
    "        # First term is a phrase, second is a single-word term\n",
    "        # Sorting only for prettier output\n",
    "        return list(\n",
    "            sorted(\n",
    "                boolean_operator(\n",
    "                    set(\n",
    "                        execute_phrase_search(\n",
    "                            phrase=\" \".join(term_1),\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                    set(\n",
    "                        execute_single_term_search(\n",
    "                            term=term_2[0],\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    elif len(term_1) == 1 and len(term_2) == 2:\n",
    "        # First term is a single-word term, second is a phrase\n",
    "        # Sorting only for prettier output\n",
    "        return list(\n",
    "            sorted(\n",
    "                boolean_operator(\n",
    "                    set(\n",
    "                        execute_single_term_search(\n",
    "                            term=term_1[0],\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                    set(\n",
    "                        execute_phrase_search(\n",
    "                            phrase=\" \".join(term_2),\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    elif len(term_1) == 2 and len(term_2) == 2:\n",
    "        # Both terms are phrases\n",
    "        # Sorting only for prettier output\n",
    "        return list(\n",
    "            sorted(\n",
    "                boolean_operator(\n",
    "                    set(\n",
    "                        execute_phrase_search(\n",
    "                            phrase=\" \".join(term_1),\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                    set(\n",
    "                        execute_phrase_search(\n",
    "                            phrase=\" \".join(term_2),\n",
    "                            inverted_index=inverted_index,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Token number is wrong\")\n",
    "\n",
    "\n",
    "\n",
    "def write_boolean_search_results(\n",
    "    inverted_index: dict,\n",
    "    boolean_queries: list,\n",
    "    file_name: str = \"results.boolean.txt\",\n",
    "    verbose: bool = False,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Executes a list of Boolean queries and writes the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    inverted_index : dict\n",
    "        The inverted index in which to perform the search.\n",
    "\n",
    "    boolean_queries : list\n",
    "        List of Boolean queries. Each query is a tuple where the first\n",
    "        element specifies the type of query ('TERM', 'PROXIMITY', 'PHRASE',\n",
    "        or Boolean operator), and the second element contains the query parameters.\n",
    "\n",
    "    file_name : str, optional (default=\"results.boolean.txt\")\n",
    "        Name of the file where the search results will be written.\n",
    "\n",
    "    verbose : bool, optional (default=False)\n",
    "        If True, prints additional information during the operation.\n",
    "\n",
    "    contraction_expansion : bool, optional (default=True)\n",
    "        Whether or not to expand contractions in the search terms.\n",
    "\n",
    "    stopword_removal : bool, optional (default=True)\n",
    "        Whether or not to remove stopwords from the search terms.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "\n",
    "    Writes:\n",
    "    -------\n",
    "    A text file where each line represents the document IDs retrieved for each query.\n",
    "    The format is \"query_index,document_id\\n\".\n",
    "    \"\"\"\n",
    "    # Open the file to write the results\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # Loop through each boolean query\n",
    "        for query_index, boolean_query in enumerate(\n",
    "            boolean_queries, start=1\n",
    "        ):  # start=1 to start query_index at 1\n",
    "            # For verbose mode, print the current query\n",
    "            if verbose:\n",
    "                print(f\"Executing query {query_index}: {boolean_query}\")\n",
    "            # Single term\n",
    "            if boolean_query[0] == \"TERM\":\n",
    "                current_term = boolean_query[1]\n",
    "                retrieved_docids = execute_single_term_search(\n",
    "                    term=current_term,\n",
    "                    inverted_index=inverted_index,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "            # Proximity search\n",
    "            elif boolean_query[0] == \"PROXIMITY\":\n",
    "                current_term_1, current_term_2, current_distance = (\n",
    "                    boolean_query[1][0][0],\n",
    "                    boolean_query[1][0][1],\n",
    "                    boolean_query[1][1],\n",
    "                )\n",
    "                retrieved_docids = execute_proximity_search(\n",
    "                    term_1=current_term_1,\n",
    "                    term_2=current_term_2,\n",
    "                    distance=current_distance,\n",
    "                    inverted_index=inverted_index,\n",
    "                    strictly_ordering_tokens=False,\n",
    "                    verbose=verbose,\n",
    "                    full_output=False,\n",
    "                )\n",
    "            # Phrase search\n",
    "            elif boolean_query[0] == \"PHRASE\":\n",
    "                current_phrase = boolean_query[1]\n",
    "                retrieved_docids = execute_phrase_search(\n",
    "                    phrase=current_phrase,\n",
    "                    inverted_index=inverted_index,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "            # Boolean search\n",
    "            else:\n",
    "                current_operator, current_term_1, current_term_2 = (\n",
    "                    boolean_query[0],\n",
    "                    boolean_query[1][0],\n",
    "                    boolean_query[1][1],\n",
    "                )\n",
    "                retrieved_docids = execute_logical_search(\n",
    "                    operator_name=current_operator,\n",
    "                    term_1=current_term_1,\n",
    "                    term_2=current_term_2,\n",
    "                    inverted_index=inverted_index,\n",
    "                    verbose=verbose,\n",
    "                    contraction_expansion=contraction_expansion,\n",
    "                    stopword_removal=stopword_removal,\n",
    "                )\n",
    "            # Write the retrieved doc IDs to the file\n",
    "            for doc_id in retrieved_docids:\n",
    "                file.write(f\"{query_index},{doc_id}\\n\")\n",
    "\n",
    "    print(f\"Boolean search results written to file {file_name}.\\n\")\n",
    "\n",
    "\n",
    "def execute_tfidf_search(\n",
    "    query: str,\n",
    "    inverted_index: dict,\n",
    "    id_to_headline_dict: dict,\n",
    "    simple_score: bool = True,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Executes a TF-IDF based search on a document collection, given a query, matching\n",
    "    documents that contain *at least one* of the query terms. Either based on simple\n",
    "    TF-IDF aggregation or cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query as a string.\n",
    "    - inverted_index (dict): Dictionary containing the inverted index. \n",
    "      The keys are terms and the values are dictionaries with 'Postings' and 'Document Frequency'.\n",
    "    - id_to_headline_dict (dict): Dictionary mapping document IDs to their headlines/titles.\n",
    "    - simple_score (bool, optional): Flag to determine the type of scoring. If True, a simple aggregated TF-IDF \n",
    "      score is calculated for each document (lect. 7, slide 15). If False, cosine similarity is used.\n",
    "      Defaults to True.\n",
    "    - contraction_expansion (bool, optional): Flag to determine if contractions should be expanded. Defaults to True.\n",
    "    - stopword_removal (bool, optional): Flag to determine if stopwords should be removed. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A sorted dictionary where keys are document IDs and values are their respective scores, sorted in \n",
    "      descending order of scores.\n",
    "    \"\"\"\n",
    "    # Get the number of documents in the collection\n",
    "    collection_size = len(id_to_headline_dict.keys())\n",
    "\n",
    "    query_token_list = get_processed_tokens_from_string(\n",
    "        query, contraction_expansion, stopword_removal\n",
    "    )\n",
    "\n",
    "    # If the query consists of only one term\n",
    "    if len(query_token_list) == 1:\n",
    "        union_doc_ids = set(\n",
    "            execute_single_term_search(\n",
    "                query_token_list[0],\n",
    "                inverted_index,\n",
    "                verbose=False,\n",
    "                stopword_removal=stopword_removal,\n",
    "            )\n",
    "        )\n",
    "    # If the query has multiple terms\n",
    "    else:\n",
    "        # Initialize set of document IDs containing the first term\n",
    "        union_doc_ids = set(\n",
    "            execute_single_term_search(\n",
    "                query_token_list[0],\n",
    "                inverted_index,\n",
    "                verbose=False,\n",
    "                stopword_removal=stopword_removal,\n",
    "            )\n",
    "        )\n",
    "        # Union with document IDs containing remaining terms\n",
    "        for term in query_token_list[1:]:\n",
    "            term_doc_ids = set(\n",
    "                execute_single_term_search(\n",
    "                    term,\n",
    "                    inverted_index,\n",
    "                    verbose=False,\n",
    "                    stopword_removal=stopword_removal,\n",
    "                )\n",
    "            )\n",
    "            union_doc_ids = operator.or_(union_doc_ids, term_doc_ids)\n",
    "    # If using simple TF-IDF scoring\n",
    "    if simple_score:\n",
    "        # Initialize document score dictionary\n",
    "        document_score_dictionary = {doc_id: 0.0 for doc_id in union_doc_ids}\n",
    "        # Calculate simple TF-IDF score for each document\n",
    "        for union_doc_id in union_doc_ids:\n",
    "            for token in query_token_list:\n",
    "                token_term_frequency = len(\n",
    "                    inverted_index[token][\"Postings\"][union_doc_id]\n",
    "                )\n",
    "                document_frequency = inverted_index[token][\"Document Frequency\"]\n",
    "                # If the term frequency is zero, then skip this term\n",
    "                if token_term_frequency == 0:\n",
    "                    continue\n",
    "                # Update the document's TF-IDF score\n",
    "                document_score_dictionary[union_doc_id] += (\n",
    "                    1 + np.log10(token_term_frequency)\n",
    "                ) * np.log10(collection_size / document_frequency)\n",
    "        # Sort by score in descending order\n",
    "        sorted_document_score_dictionary = {\n",
    "            k: v\n",
    "            for k, v in sorted(\n",
    "                document_score_dictionary.items(),\n",
    "                key=lambda item: item[1],\n",
    "                reverse=True,\n",
    "            )\n",
    "        }\n",
    "        return sorted_document_score_dictionary\n",
    "    \n",
    "    # If using cosine similarity scoring:\n",
    "\n",
    "    # 1. Here I calculate the document magnitude with TF-IDF, 1:1 as in\n",
    "    # page 125 of the Introduction to Information Retrieval book, as far\n",
    "    # as I understand it.\n",
    "\n",
    "    else:\n",
    "        def compute_document_magnitude(doc_id, inverted_index, collection_size):\n",
    "            magnitude = 0.0\n",
    "            # Iterate over all terms in the document\n",
    "            for term, posting_data in inverted_index.items():\n",
    "                if doc_id in posting_data[\"Postings\"]:\n",
    "                    tf = len(posting_data[\"Postings\"][doc_id])\n",
    "                    idf = np.log10(collection_size / posting_data[\"Document Frequency\"])\n",
    "                    magnitude += (tf * idf) ** 2\n",
    "            return np.sqrt(magnitude)\n",
    "        \n",
    "        # Initialize scores for all documents\n",
    "        Scores = {doc_id: 0.0 for doc_id in union_doc_ids}\n",
    "        Length = {doc_id: 0.0 for doc_id in union_doc_ids}\n",
    "        \n",
    "        query_term_frequencies = Counter(query_token_list)\n",
    "        \n",
    "        # For each query term, update the Scores array\n",
    "        for token in query_token_list:\n",
    "            # calculate wt_q, i.e. for query, and fetch postings list\n",
    "            wt_q = (\n",
    "                (1 + np.log10(query_term_frequencies[token]))\n",
    "                * np.log10(collection_size / inverted_index[token][\"Document Frequency\"])\n",
    "            )\n",
    "            postings_list = list(inverted_index[token][\"Postings\"].keys())\n",
    "            for doc_id in postings_list:\n",
    "                if doc_id not in union_doc_ids:\n",
    "                    continue\n",
    "                token_term_frequency = len(inverted_index[token][\"Postings\"][doc_id])\n",
    "                wft_d = (\n",
    "                    (1 + np.log10(token_term_frequency))\n",
    "                    * np.log10(collection_size / inverted_index[token][\"Document Frequency\"])\n",
    "                )\n",
    "                Scores[doc_id] += wft_d * wt_q\n",
    "                Length[doc_id] += wft_d**2\n",
    "        \n",
    "        # Normalize the Scores by the vector length of each document\n",
    "        for doc_id in union_doc_ids:\n",
    "            Scores[doc_id] /= compute_document_magnitude(doc_id, inverted_index, collection_size)\n",
    "        \n",
    "        # Sort by score in descending order\n",
    "        sorted_scores = {\n",
    "            k: v\n",
    "            for k, v in sorted(Scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        }\n",
    "        \n",
    "        return sorted_scores\n",
    "\n",
    "    # 2. Here I just do it intuitively, NOT penalising the excessive\n",
    "    # length of a document. I expected this to be give exactly the same results\n",
    "    # simple_score=True. I don't know why that's not the case.\n",
    "\n",
    "    # else:\n",
    "    #     # Initialize variables for vector space model\n",
    "    #     query_dimensionality = len(query_token_list)\n",
    "    #     document_score_dictionary = {}\n",
    "\n",
    "    #     query_term_frequencies = Counter(query_token_list)\n",
    "\n",
    "    #     query_tfidf_vector = np.array(\n",
    "    #         [\n",
    "    #             (1 + np.log10(query_term_frequencies[token]))\n",
    "    #             * np.log10(\n",
    "    #                 collection_size / inverted_index[token][\"Document Frequency\"]\n",
    "    #             )\n",
    "    #             for token in query_token_list\n",
    "    #         ]\n",
    "    #     )\n",
    "\n",
    "    #     query_tfidf_vector /= np.linalg.norm(query_tfidf_vector)\n",
    "\n",
    "    #     # Calculate TF-IDF vector for each document\n",
    "    #     for union_doc_id in union_doc_ids:\n",
    "    #         # Initialize a zero vector for each document\n",
    "\n",
    "    #         doc_tfidf_vector = np.zeros(query_dimensionality)\n",
    "\n",
    "    #         for index, token in enumerate(query_token_list):\n",
    "    #             token_term_frequency = len(\n",
    "    #                 inverted_index[token][\"Postings\"][union_doc_id]\n",
    "    #             )\n",
    "    #             document_frequency = inverted_index[token][\"Document Frequency\"]\n",
    "\n",
    "    #             # If the term frequency is zero, then skip this term\n",
    "    #             if token_term_frequency == 0:\n",
    "    #                 continue\n",
    "    #             # Update the document's TF-IDF vector\n",
    "    #             token_value_tfidf_in_document_vector = (\n",
    "    #                 1 + np.log10(token_term_frequency)\n",
    "    #             ) * np.log10(collection_size / document_frequency)\n",
    "\n",
    "    #             doc_tfidf_vector[index] = token_value_tfidf_in_document_vector\n",
    "    #         # Normalize the vector to have L2 norm = 1\n",
    "    #         l2_norm = np.linalg.norm(doc_tfidf_vector)\n",
    "    #         if l2_norm > 0:  # To avoid division by zero\n",
    "    #             doc_tfidf_vector /= l2_norm\n",
    "    #         # Store the normalized vector in the dictionary\n",
    "    #         document_score_dictionary[union_doc_id] = doc_tfidf_vector\n",
    "    #     # Calculate dot products for each document\n",
    "    #     doc_id_to_dot_product = {\n",
    "    #         doc_id: np.dot(query_tfidf_vector, tfidf_vector)\n",
    "    #         for doc_id, tfidf_vector in document_score_dictionary.items()\n",
    "    #     }\n",
    "\n",
    "    #     # Sort the dictionary by dot product in descending order\n",
    "    #     sorted_doc_id_to_dot_product = {\n",
    "    #         k: v\n",
    "    #         for k, v in sorted(\n",
    "    #             doc_id_to_dot_product.items(), key=lambda item: item[1], reverse=True\n",
    "    #         )\n",
    "    #     }\n",
    "    #     return sorted_doc_id_to_dot_product\n",
    "\n",
    "    \n",
    "\n",
    "def get_ranked_query_list(query_file_path: str, chars_to_skip: int = 2) -> list:\n",
    "    \"\"\"\n",
    "    Reads a text file containing queries and returns a list of queries.\n",
    "\n",
    "    Parameters:\n",
    "    - query_file_path (str): The file path to the text file containing the queries.\n",
    "    - chars_to_skip (int, optional): The number of characters to skip at the beginning \n",
    "      of each line to get to the query text. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of queries read from the file, with unwanted characters removed.\n",
    "\n",
    "    Example:\n",
    "    Given a text file ('queries.txt') with content like:\n",
    "    1 income tax reduction\n",
    "    2 stock market in Japan\n",
    "\n",
    "    get_ranked_query_list('queries.txt', 2) will return ['income tax reduction', 'stock market in Japan'].\n",
    "    \"\"\"\n",
    "    queries = open(query_file_path).read().split(\"\\n\")\n",
    "    query_list = [\n",
    "        (queries[idx][chars_to_skip:])\n",
    "        for idx in range(len((queries)))\n",
    "    ]\n",
    "    return [item for item in query_list if item is not None and item != '']\n",
    "\n",
    "\n",
    "def write_ranked_search_results(\n",
    "    inverted_index: dict,\n",
    "    id_to_headline_dict: dict,\n",
    "    ranked_queries: list,\n",
    "    file_name: str = \"results.ranked.txt\",\n",
    "    simple_score: bool = True,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ")->None:\n",
    "    \"\"\"\n",
    "    Executes TF-IDF based searches for each query in 'ranked_queries', and writes the \n",
    "    top 150 results to a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - inverted_index (dict): Dictionary containing the inverted index.\n",
    "    - id_to_headline_dict (dict): Dictionary mapping document IDs to their headlines/titles.\n",
    "    - ranked_queries (list): List of preprocessed queries to search against the collection.\n",
    "    - file_name (str, optional): The name of the file where the search results will be written. \n",
    "      Defaults to \"results.ranked.txt\".\n",
    "    - simple_score (bool, optional): Flag to determine the type of scoring. Defaults to True.\n",
    "    - contraction_expansion (bool, optional): Flag to determine if contractions should be expanded. \n",
    "      Defaults to True.\n",
    "    - stopword_removal (bool, optional): Flag to determine if stopwords should be removed. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function writes to a file and does not return any value.\n",
    "\n",
    "    Output Format:\n",
    "    Writes to a text file where each line is of the form: <Query Index>,<Document ID>,<Score>\n",
    "    For example, '1,doc1,0.1234\\n'.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as out_file:\n",
    "        # Loop through each query\n",
    "        for index, current_ranked_query in enumerate(\n",
    "            ranked_queries, start=1\n",
    "        ):\n",
    "            # Get the results of the TF-IDF search\n",
    "            current_tfidf_results = execute_tfidf_search(\n",
    "                current_ranked_query,\n",
    "                inverted_index,\n",
    "                id_to_headline_dict,\n",
    "                simple_score=simple_score,\n",
    "                contraction_expansion=contraction_expansion,\n",
    "                stopword_removal=stopword_removal,\n",
    "            )\n",
    "            # Limit results to top 150\n",
    "            for rank, (doc_id, score) in enumerate(current_tfidf_results.items()):\n",
    "                if rank >= 150:\n",
    "                    break\n",
    "                # Round score to four decimal places\n",
    "                rounded_score = round(score, 4)\n",
    "                # Write to the file\n",
    "                out_file.write(f\"{index},{doc_id},{rounded_score}\\n\")\n",
    "    print(f\"Ranked search results written to file {file_name}.\\n\")\n",
    "\n",
    "\n",
    "def perform_boolean_experiment(\n",
    "    inverted_index_path: str = \"inverted_index.pkl\",\n",
    "    id_to_headline_path: str = \"id_to_headline_dict.pkl\",\n",
    "    query_file_path: str = \"queries.boolean.txt\",\n",
    "    chars_to_skip: int = 2,\n",
    "    verbose: bool = False,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs a full Boolean search experiment using the given inverted index and query file.\n",
    "    Writes the results to a text file.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    - inverted_index_path (str): The file path to the pickled inverted index.\n",
    "      Defaults to \"inverted_index.pkl\".\n",
    "    - id_to_headline_path (str): The file path to the pickled id_to_headline dictionary.\n",
    "      Defaults to \"id_to_headline_dict.pkl\".\n",
    "    - query_file_path (str): The file path to the text file containing the queries.\n",
    "      Defaults to \"queries.boolean.txt\".\n",
    "    - chars_to_skip (int, optional): The number of characters to skip at the beginning\n",
    "      of each line to get to the query text. Defaults to 2.\n",
    "    - verbose (bool): If True, prints additional information during the operation. Defaults to False.\n",
    "    - contraction_expansion (bool): Flag to determine if contractions should be expanded for queries.\n",
    "      Defaults to True.\n",
    "    - stopword_removal (bool): Flag to determine if stopwords should be removed. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of queries read from the file, with unwanted characters removed.\n",
    "    \"\"\"\n",
    "\n",
    "    current_inverted_index, _ = load_pickled_data(\n",
    "        inverted_index_path=inverted_index_path,\n",
    "        id_to_headline_path=id_to_headline_path,\n",
    "    )\n",
    "\n",
    "    current_boolean_query_list = get_boolean_query_list(\n",
    "        query_file_path=query_file_path, chars_to_skip=chars_to_skip\n",
    "    )\n",
    "\n",
    "    write_boolean_search_results(\n",
    "        current_inverted_index,\n",
    "        current_boolean_query_list,\n",
    "        verbose=verbose,\n",
    "        contraction_expansion=contraction_expansion,\n",
    "        stopword_removal=stopword_removal,\n",
    "    )\n",
    "\n",
    "\n",
    "def perform_ranked_experiment(\n",
    "    inverted_index_path: str = \"inverted_index.pkl\",\n",
    "    id_to_headline_path: str = \"id_to_headline_dict.pkl\",\n",
    "    query_file_path: str = \"queries.ranked.txt\",\n",
    "    chars_to_skip: int = 2,\n",
    "    simple_score: bool = True,\n",
    "    contraction_expansion: bool = True,\n",
    "    stopword_removal: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs a full TFIDF  search experiment using the given inverted index and query file.\n",
    "    Writes the results to a text file.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    - inverted_index_path (str): The file path to the pickled inverted index.\n",
    "      Defaults to \"inverted_index.pkl\".\n",
    "    - id_to_headline_path (str): The file path to the pickled id_to_headline dictionary.\n",
    "      Defaults to \"id_to_headline_dict.pkl\".\n",
    "    - query_file_path (str): The file path to the text file containing the queries.\n",
    "      Defaults to \"queries.ranked.txt\".\n",
    "    - chars_to_skip (int, optional): The number of characters to skip at the beginning\n",
    "      of each line to get to the query text. Defaults to 2.\n",
    "    - simple_score (bool, optional): Flag to determine the type of scoring. Defaults to True (aggregate TF-IDF).\n",
    "    - contraction_expansion (bool): Flag to determine if contractions should be expanded for queries.\n",
    "      Defaults to True.\n",
    "    - stopword_removal (bool): Flag to determine if stopwords should be removed. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of queries read from the file, with unwanted characters removed.\n",
    "    \"\"\"\n",
    "\n",
    "    current_inverted_index, current_id_to_headline_dict = load_pickled_data(\n",
    "        inverted_index_path=inverted_index_path,\n",
    "        id_to_headline_path=id_to_headline_path,\n",
    "    )\n",
    "\n",
    "    current_ranked_query_list = get_ranked_query_list(\n",
    "        query_file_path=query_file_path, chars_to_skip=chars_to_skip\n",
    "    )\n",
    "\n",
    "    write_ranked_search_results(\n",
    "        current_inverted_index,\n",
    "        current_id_to_headline_dict,\n",
    "        current_ranked_query_list,\n",
    "        simple_score=simple_score,\n",
    "        contraction_expansion=contraction_expansion,\n",
    "        stopword_removal=stopword_removal,\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main of the search engine.\n",
    "\n",
    "    This function parses command line arguments to run functionalities:\n",
    "    building an index, performing a boolean search, or executing a ranked search.\n",
    "\n",
    "    Example Usage:\n",
    "        1. To build the index:\n",
    "            python code.py --mode build_index\n",
    "        2. To perform a boolean search:\n",
    "            python code.py --mode boolean_search --verbose True\n",
    "        3. To perform a ranked search with custom parameters:\n",
    "            python code.py --mode ranked_search --verbose True\n",
    "\n",
    "    Parameters can also be combined and/or changed. For example:\n",
    "        python code.py --mode build_index --xml-path \"custom.xml\" --csv False\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Search Engine\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--mode\",\n",
    "        choices=[\"build_index\", \"boolean_search\", \"ranked_search\"],\n",
    "        help=\"build_index: create_and_save_index, boolean_search: perform_boolean_experiment, ranked_search: perform_ranked_experiment\",\n",
    "    )\n",
    "\n",
    "    # Common options\n",
    "    parser.add_argument(\n",
    "        \"--contraction-expansion\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Flag for every mode to determine if contractions should be expanded during preprocessing and index construction. Default is True.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--stopword-removal\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Flag for every mode determine if stopwords should be removed during preprocessing and index construction. Default is True.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"Flag for search modes. If True, prints additional information during both boolean and ranked searches. Default is False.\",\n",
    "    )\n",
    "\n",
    "    # Building options\n",
    "    parser.add_argument(\n",
    "        \"--xml-path\",\n",
    "        type=str,\n",
    "        default=\"trec.5000.xml\",\n",
    "        help=\"The path for build mode to the XML file containing the documents to index. Default is 'trec.5000.xml'.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--csv\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Whether to save the index to a CSV file in build mode. Default is True.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--txt\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Whether to save the index to a text file in build mode. Default is True.\",\n",
    "    )\n",
    "\n",
    "    # Common search options\n",
    "    parser.add_argument(\n",
    "        \"--inverted-index-path\",\n",
    "        type=str,\n",
    "        default=\"inverted_index.pkl\",\n",
    "        help=\"The file path to the pickled inverted index for search modes. Default is 'inverted_index.pkl'.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--id-to-headline-path\",\n",
    "        type=str,\n",
    "        default=\"id_to_headline_dict.pkl\",\n",
    "        help=\"The file path to the pickled id_to_headline dictionary for search modes. Default is 'id_to_headline_dict.pkl'.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chars-to-skip\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"The number of characters for search modes to skip at the beginning of each line to get to the query text. Default is 2.\",\n",
    "    )\n",
    "\n",
    "    # Boolean search options\n",
    "    parser.add_argument(\n",
    "        \"--boolean-queries-path\",\n",
    "        type=str,\n",
    "        default=\"queries.boolean.txt\",\n",
    "        help=\"The file path for boolean search mode to the text file containing the boolean queries. Default is 'queries.boolean.txt'.\",\n",
    "    )\n",
    "\n",
    "    # Ranked search options\n",
    "    parser.add_argument(\n",
    "        \"--ranked-queries-path\",\n",
    "        type=str,\n",
    "        default=\"queries.ranked.txt\",\n",
    "        help=\"The file path for ranked search mode to the text file containing the ranked queries. Default is 'queries.ranked.txt'.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"build_index\":\n",
    "        create_and_save_index(\n",
    "            xml_path=args.xml_path,\n",
    "            contraction_expansion=args.contraction_expansion,\n",
    "            stopword_removal=args.stopword_removal,\n",
    "            csv=args.csv,\n",
    "            txt=args.txt,\n",
    "        )\n",
    "    elif args.mode == \"boolean_search\":\n",
    "        perform_boolean_experiment(\n",
    "            inverted_index_path=args.inverted_index_path,\n",
    "            id_to_headline_path=args.id_to_headline_path,\n",
    "            query_file_path=args.boolean_queries_path,\n",
    "            chars_to_skip=args.chars_to_skip,\n",
    "            verbose=args.verbose,\n",
    "            contraction_expansion=args.contraction_expansion,\n",
    "            stopword_removal=args.stopword_removal,\n",
    "        )\n",
    "    elif args.mode == \"ranked_search\":\n",
    "        perform_ranked_experiment(\n",
    "            inverted_index_path=args.inverted_index_path,\n",
    "            id_to_headline_path=args.id_to_headline_path,\n",
    "            query_file_path=args.ranked_queries_path,\n",
    "            chars_to_skip=args.chars_to_skip,\n",
    "            simple_score=True,\n",
    "            contraction_expansion=args.contraction_expansion,\n",
    "            stopword_removal=args.stopword_removal,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Invalid option chosen. Use '-h' or '--help' for usage information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing documents:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abfe0747c6b4ecfb997d626bad180cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorting index...\n",
      "Index sorting took 0.24 seconds.\n",
      "Final index has 13970 terms and took 10.62 seconds to construct.\n",
      "Inverted index saved to file 'inverted_index.pkl'.\n",
      "ID to headline dictionary saved to file 'id_to_headline_dict.pkl'.\n",
      "Ranked search results written to file results.ranked.txt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XML_PATH = \"trec.sample.xml\"\n",
    "BOOLEAN_QUERIES_PATH = \"queries.boolean.txt\"\n",
    "RANKED_QUERIES_PATH = \"queries.ranked.txt\"\n",
    "\n",
    "INVERTED_INDEX_PATH = \"inverted_index.pkl\"\n",
    "ID_TO_HEADLINE_DICT_PATH = \"id_to_headline_dict.pkl\"\n",
    "\n",
    "CONTRACTION_EXPANSION = True\n",
    "STOPWORD_REMOVAL = True\n",
    "CSV = False\n",
    "TXT = False\n",
    "CHARS_TO_SKIP = 4\n",
    "VERBOSE = False\n",
    "SIMPLE_TFIDF_SCORE = True\n",
    "\n",
    "\n",
    "# Functionality 1\n",
    "create_and_save_index(\n",
    "    xml_path=XML_PATH,\n",
    "    contraction_expansion=CONTRACTION_EXPANSION,\n",
    "    stopword_removal=STOPWORD_REMOVAL,\n",
    "    csv=CSV,\n",
    "    txt=TXT,\n",
    ")\n",
    "\n",
    "# # Functionality 2\n",
    "# perform_boolean_experiment(\n",
    "#     inverted_index_path=INVERTED_INDEX_PATH,\n",
    "#     id_to_headline_path=ID_TO_HEADLINE_DICT_PATH,\n",
    "#     query_file_path=BOOLEAN_QUERIES_PATH,\n",
    "#     chars_to_skip=CHARS_TO_SKIP,\n",
    "#     verbose=VERBOSE,\n",
    "#     contraction_expansion=CONTRACTION_EXPANSION,\n",
    "#     stopword_removal=STOPWORD_REMOVAL,\n",
    "# )\n",
    "\n",
    "# Functionality 3\n",
    "perform_ranked_experiment(\n",
    "    inverted_index_path=INVERTED_INDEX_PATH,\n",
    "    id_to_headline_path=ID_TO_HEADLINE_DICT_PATH,\n",
    "    query_file_path=RANKED_QUERIES_PATH,\n",
    "    chars_to_skip=CHARS_TO_SKIP,\n",
    "    simple_score=SIMPLE_TFIDF_SCORE,\n",
    "    contraction_expansion=CONTRACTION_EXPANSION,\n",
    "    stopword_removal=STOPWORD_REMOVAL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'income tax reduction ': 1,\n",
       "         'peace in the Middle East': 1,\n",
       "         'unemployment rate in UK': 1,\n",
       "         'industry in scotland': 1,\n",
       "         'the industries of computers': 1,\n",
       "         'Microsoft Windows': 1,\n",
       "         'stock market in Japan': 1,\n",
       "         'the education with computers': 1,\n",
       "         'health industry': 1,\n",
       "         ' campaigns of political parties': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_ranked_query_list = get_ranked_query_list(\n",
    "    query_file_path=RANKED_QUERIES_PATH, chars_to_skip=2\n",
    ")\n",
    "\n",
    "(current_ranked_query_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
